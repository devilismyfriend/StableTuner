{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Welcome to StableTuner, Let's get started!\n",
    "#### This notebook will guide you through the setup process.\n",
    "\n",
    "\n",
    "__[Join the ST Discord for support, chat and fun times :)](https://discord.gg/DahNECrBUZ)__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by uploading your payload.zip file (just drag and drop it to the file area) and run this cell as it gets uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Much thanks to IndustriaDitat and entmike for helping making ST linux compatible!\n",
    "from IPython.display import clear_output\n",
    "from subprocess import getoutput\n",
    "installed_xformers = False\n",
    "GPU_CardName = getoutput('nvidia-smi --query-gpu=name --format=csv,noheader')\n",
    "\n",
    "%pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url \"https://download.pytorch.org/whl/cu116\"\n",
    "%pip install -U --pre triton\n",
    "%pip install ninja bitsandbytes\n",
    "%pip install -q gradio\n",
    "if '3090' in GPU_CardName:\n",
    "    %pip install https://huggingface.co/industriaditat/xformers_precompiles/resolve/main/RTX3090-xf14-cu116-py38/xformers-0.0.14.dev0-cp38-cp38-linux_x86_64.whl\n",
    "    installed_xformers = True\n",
    "if 'A5000' in GPU_CardName:\n",
    "    %pip install https://huggingface.co/industriaditat/xformers_precompiles/resolve/main/A5000-xf14-cu116-py38/xformers-0.0.14.dev0-cp38-cp38-linux_x86_64.whl\n",
    "    installed_xformers = True\n",
    "if 'T4' in GPU_CardName:\n",
    "    %pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/xformers-0.0.14.dev0-cp38-cp38-linux_x86_64_t4.whl\n",
    "    installed_xformers = True\n",
    "if 'A100' in GPU_CardName:\n",
    "    %pip install https://huggingface.co/industriaditat/xformers_precompiles/resolve/main/A100_13dev/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "    installed_xformers = True\n",
    "if 'V100' in GPU_CardName:\n",
    "    %pip install https://huggingface.co/industriaditat/xformers_precompiles/resolve/main/V100_13dev/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "    installed_xformers = True\n",
    "if installed_xformers == False:\n",
    "    clear_output()\n",
    "    print(\"No precompiled xformers found for your GPU. Please wait while we compile xformers for your GPU, this might take 20-40 minutes.\")\n",
    "    %pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers\n",
    "%pip install git+https://github.com/huggingface/diffusers.git@0ca1724#egg=diffusers --force-reinstall\n",
    "clear_output()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload finished?, time to run this next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o -q payload.zip\n",
    "%pip install -r requirements.txt\n",
    "clear_output()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like you're done installing, let's get training!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeMe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Playground\n",
    "\n",
    "#### This is where you can test your model and package it up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this next cell and select your output model, we'll automatically load the latest checkpoint for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from IPython.display import display\n",
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        print('Loading new model')\n",
    "        model_path = change['new'] \n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_path,safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        pipe.scheduler = scheduler\n",
    "        print(\"Checkpoint changed to %s\" % change['new'])\n",
    "        \n",
    "\n",
    "if 'output' not in os.listdir():\n",
    "    print(\"No output folder found. Please run the training cell first.\")\n",
    "models = []\n",
    "model_dir = os.listdir('output')[0]\n",
    "output_sort = sorted(glob.iglob('output' + os.sep + model_dir + os.sep+ '*'), key=os.path.getctime, reverse=True)\n",
    "if len(output_sort) == 0:\n",
    "    print(\"No models found in output folder. Please run the training cell first.\")\n",
    "for model in output_sort:\n",
    "    required_folders = [\"vae\", \"unet\", \"tokenizer\", \"text_encoder\"]\n",
    "    if all(x in os.listdir(model) for x in required_folders):\n",
    "        models.append(model)\n",
    "model_selection = widgets.Dropdown(\n",
    "    layout={'width': 'initial'},\n",
    "    style={'description_width': 'initial'},\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    # rows=10,\n",
    "    description='Select Checkpoint:',\n",
    "    disabled=False\n",
    ")\n",
    "model_selection.observe(on_change)\n",
    "clear_output()\n",
    "display(model_selection)\n",
    "model_path = model_selection.value\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path,safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.scheduler = scheduler\n",
    "print('Loaded last checkpoint')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ready?, let's play around with it ;), this next cell will load a small UI for you to generate images and zip it up for download, re-run this cell if you selected a new model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50,seed=-1,guidance_scale=7.5):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "      if seed != -1:\n",
    "        g_cuda = torch.Generator(device='cuda')\n",
    "        g_cuda.manual_seed(int(seed))\n",
    "      else:\n",
    "        seed = random.randint(0, 100000)\n",
    "        g_cuda = torch.Generator(device='cuda')\n",
    "        g_cuda.manual_seed(seed)\n",
    "        return pipe(\n",
    "                prompt, height=int(height), width=int(width),\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_images_per_prompt=int(num_samples),\n",
    "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                generator=g_cuda\n",
    "            ).images, seed\n",
    "def zip_model():\n",
    "  import shutil\n",
    "  print('Zipping Model!, Please wait until you see a done message, this can take a few minutes, you can keep generating while you wait!')\n",
    "  curLocation = os.getcwd()\n",
    "  model_name = os.path.dirname(model_path)\n",
    "  shutil.make_archive(model_name,'zip',model_path)\n",
    "  os.chdir(curLocation)\n",
    "  print('Done!')\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            with gr.Row():\n",
    "              run = gr.Button(value=\"Generate\")\n",
    "              zip = gr.Button(value=\"Zip Model For Download\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            with gr.Row():\n",
    "              num_inference_steps = gr.Slider(label=\"Steps\", value=25)\n",
    "              seed = gr.Number(label=\"Seed\", value=-1)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "            seedDisplay = gr.Number(label=\"Used Seed:\", value=0)\n",
    "\n",
    "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps,seed, guidance_scale], outputs=[gallery,seedDisplay])\n",
    "    zip.click(zip_model)\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end, for now :) ,you can convert your model to CKPT back in StableTuner!."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "886cb931ea414ad2a87adcccbb1ce9166879eb6056301acd331591c6290ceca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
